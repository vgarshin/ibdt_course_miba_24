{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457f84e1",
   "metadata": {},
   "source": [
    "# Introduction to Big Data Modern Technologies course"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d807a92",
   "metadata": {},
   "source": [
    "## TOPIC 4: Modern Hadoop\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22528293",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92b729d-5eb1-4b90-929b-96a2fc61233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7233a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a485bcf2",
   "metadata": {},
   "source": [
    "### 2. Raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0cfe23",
   "metadata": {},
   "source": [
    "#### 2.1. Data to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la ~/__DATA/IBDT_Spring_2024/topic_4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742d7e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9163bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -R /jovyan/raw\n",
    "!hdfs dfs -rm -R /jovyan/users\n",
    "!hdfs dfs -rm -R /jovyan/instances\n",
    "!hdfs dfs -rm -R /jovyan/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39839a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /jovyan/raw\n",
    "!hdfs dfs -put ~/__DATA/IBDT_Spring_2024/topic_4/jhub_logs.csv /jovyan/raw/\n",
    "!hdfs dfs -ls /jovyan/raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f344f9d5",
   "metadata": {},
   "source": [
    "#### 2.2. Read data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdddb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('user:', os.environ['JUPYTERHUB_SERVICE_PREFIX'])\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf().set('spark.master', 'local[*]') #stand-alone mode\n",
    "#conf = SparkConf().set('spark.master', 'yarn') # YARN managed\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.read.csv(\n",
    "    f'/jovyan/raw/jhub_logs.csv',\n",
    "    sep=';', \n",
    "    header=True,\n",
    "    multiLine=True, # if you have `\\n` symbols\n",
    "    escape=\"\\\"\"\n",
    ")\n",
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362da30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is recommended to use `limit`\n",
    "# to read only first rows to convert to Pandas\n",
    "\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65ae5d",
   "metadata": {},
   "source": [
    "These two methods to be called with caution, because of calculation time and costs in a case of Spark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63215f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ded00-0c0b-4b8e-b95c-66e0a0784883",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376b76a",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1366972",
   "metadata": {},
   "source": [
    "#### 3.1. Kubernetes logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('kubernetes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412dd0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# bad practice actually, use `limit()`\n",
    "sdf.limit(5).select('kubernetes').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f141c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrong way, only for demo\n",
    "sdf.limit(5).select('log').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# again - use `limit()`\n",
    "sdf.limit(5).select('kubernetes').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75156c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_info(rin):\n",
    "    \"\"\"\n",
    "    Extracts names of:\n",
    "      - docker image\n",
    "      - id of the Jupyter application\n",
    "      - name of the host, where Jupyter runs\n",
    "    \n",
    "    \"\"\"\n",
    "    img = rin[rin.find('container_image='):].split('\\'')[1]\n",
    "    hub = rin[rin.find('pod_name='):].split('\\'')[1]\n",
    "    host = rin[rin.find('host='):].split('\\'')[1]\n",
    "    return img, hub, host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# it is slow without `limit()`...\n",
    "row = sdf.limit(5).select('kubernetes').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c7d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = list(row)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d46b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551caea",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_info(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fc0d42",
   "metadata": {},
   "source": [
    "#### 3.2. JupyterHub logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a92e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).select('log').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c619d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_brackets(sin):\n",
    "    \"\"\"\n",
    "    Split log string amd extracts:\n",
    "      - timestamp of the event\n",
    "      - name of application\n",
    "      - type of logs\n",
    "      - code of event\n",
    "      - description\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        s = sin.split('[', 1)[1].split(']')[0]\n",
    "        msg = sin[len(s) + 2 :].strip()\n",
    "        s = s.split()\n",
    "        head = s[0]\n",
    "        ts = ' '.join(s[1:3])\n",
    "        svc = s[3]\n",
    "        typ = s[4].split(':')[0]\n",
    "        code = s[4].split(':')[1]\n",
    "    except:\n",
    "        head, ts, svc, typ, code = '', '', '', '', ''\n",
    "        msg = sin\n",
    "    return head, ts, svc, typ, code, msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = sdf.limit(5).select('log').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b269a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = list(row)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e47054",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_brackets(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b682be77",
   "metadata": {},
   "source": [
    "#### 3.3. Apply function Spark style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1e4d1",
   "metadata": {},
   "source": [
    "Here we need Spark's user-defined functions or [udf](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154db43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ac0a5",
   "metadata": {},
   "source": [
    "##### Kubernetes column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8776d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_row_info = udf(row_info, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3184a1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('kubernetes_msg', udf_row_info('kubernetes'))\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bacd9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.limit(5).select('kubernetes_msg').collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    'date',\n",
    "    F.col('kubernetes_msg')[0].alias('img'),\n",
    "    F.col('kubernetes_msg')[1].alias('hub'),\n",
    "    F.col('kubernetes_msg')[2].alias('host'),\n",
    "    'log',\n",
    "    'stream',\n",
    "    'time'\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadd9bf",
   "metadata": {},
   "source": [
    "__NOTE__ the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26df47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8718b17f",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fedd027",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sdf.toPandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a90f57",
   "metadata": {},
   "source": [
    "##### Log column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e711f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_sq_brackets = udf(sq_brackets, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('log_msg', udf_sq_brackets('log'))\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83471d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    'img',\n",
    "    'hub',\n",
    "    'host',\n",
    "    F.col('log_msg')[0].alias('head'),\n",
    "    F.col('log_msg')[1].alias('timestamp'),\n",
    "    F.col('log_msg')[2].alias('service'),\n",
    "    F.col('log_msg')[3].alias('event_type'),\n",
    "    F.col('log_msg')[4].alias('event_code'),\n",
    "    F.col('log_msg')[5].alias('message')\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046222c6",
   "metadata": {},
   "source": [
    "#### 3.4. Find users' activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc68a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parce_users_activities(code, msg):\n",
    "    \"\"\"\n",
    "    Ugly function.\n",
    "    \n",
    "    You may use dictionary to make it\n",
    "    more pythonic or something else.\n",
    "    \n",
    "    \"\"\"\n",
    "    if code in ['43', '44']:\n",
    "        user = msg.split()[-1]\n",
    "        log = 'logged out'\n",
    "    elif code in ['61', '85', '111']:\n",
    "        user = msg.split()[3]\n",
    "        log = 'spawning sever with advanced configuration option'\n",
    "    elif code == '148':\n",
    "        user = msg.split()[-1]\n",
    "        log = 'user is running'\n",
    "    elif code == '167':\n",
    "        user = msg.split()[1]\n",
    "        log = 'server is already active'\n",
    "    if code == '238':\n",
    "        user = msg.split()[-1]\n",
    "        log = 'adding role for user'\n",
    "    elif code in ['257', '333']:\n",
    "        user = msg.split()[2]\n",
    "        log = 'adding user to proxy'\n",
    "    elif code in ['281', '359']:\n",
    "        user = msg.split()[2]\n",
    "        log = 'removing user from proxy'\n",
    "    elif code == '361':\n",
    "        user = msg.split()[1]\n",
    "        log = 'user requested new auth token'\n",
    "    elif code == '380':\n",
    "        user = msg.split()[3]\n",
    "        log = 'previous spawn failed'\n",
    "    elif code in ['402', '394']:\n",
    "        user = msg.split()[0]\n",
    "        log = 'pending spawn'\n",
    "    elif code == '567':\n",
    "        user = msg.split('/')[4]\n",
    "        log = 'stream closed while handling '\n",
    "    elif code == '626':\n",
    "        user = msg.split()[1]\n",
    "        log = 'server is already started'\n",
    "    elif code == '651':\n",
    "        user = msg.split('-')[2]\n",
    "        log = 'creating oauth client for user'\n",
    "    elif code in ['664', '749']:\n",
    "        user = msg.split()[1]\n",
    "        log = 'server is ready'\n",
    "    elif code == '681':\n",
    "        user = msg.split()[0].replace('\\'s', '')\n",
    "        log = 'server failed to start'\n",
    "    elif code == '689':\n",
    "        user = msg.split()[3].replace('\\'s', '')\n",
    "        log = 'unhandled error starting with timeout'\n",
    "    elif code == '738':\n",
    "        user = msg.split()[0].replace(',', '').replace('\\'s', '')\n",
    "        log = 'server never showed up and giving up'\n",
    "    elif code in ['757', '810']:\n",
    "        user = msg.split()[-1]\n",
    "        log = 'logged in'\n",
    "    elif code in ['904', '963']:\n",
    "        user = msg.split()[1]\n",
    "        log = 'server took time to start'\n",
    "    elif code in ['1067', '2022']:\n",
    "        user = msg.split()[1]\n",
    "        log = 'user server stopped with exit code 1'\n",
    "    elif code in ['1110', '1170']:\n",
    "        user = msg.split()[1]\n",
    "        log = 'server took time to stop'\n",
    "    elif code in ['1143', '1203']:\n",
    "        user = msg.split()[1].replace(':', '')\n",
    "        log = 'server is slow to stop'\n",
    "    elif code in ['1344', '1409']:\n",
    "        user = msg.split('/')[3]\n",
    "        log = 'failing suspected api request to not-running server'\n",
    "    elif code in ['1143', '1203']:\n",
    "        user = msg.split()[1].replace('/', '')\n",
    "        log = 'server is slow to stop'\n",
    "    elif code == '1415':\n",
    "        user = msg.split()[-1]\n",
    "        log = 'admin requesting spawn on behalf'\n",
    "    elif code == '1437':\n",
    "        user = msg.split()[1]\n",
    "        log = 'user requested server which user do not own'\n",
    "    elif code == '1840':\n",
    "        user = msg.split()[4].replace('jupyter-', '').replace(',', '')\n",
    "        log = 'attempting to create pod with timeout'\n",
    "    elif code == '1857':\n",
    "        user = msg.split()[3].replace('jupyter-', '').replace(',', '')\n",
    "        log = 'found existing pod and attempting to kill'\n",
    "    elif code == '1861':\n",
    "        user = msg.split()[2].replace('jupyter-', '').replace(',', '')\n",
    "        log = 'killed pod and will try starting singleuser pod again'\n",
    "    elif code in ['1875', '2469', '2509']:\n",
    "        user = msg.split()[4].replace('claim-', '').replace('jupyter-', '').replace(',', '')\n",
    "        log = 'attempt to create pvc with timeout'\n",
    "    elif code in ['1887', '2525']:\n",
    "        user = msg.split()[1].replace('claim-', '')\n",
    "        log = 'pvc already exists'\n",
    "    elif code in ['1961', '2044']:\n",
    "        user = msg.split()[1].replace('jupyter-', '')\n",
    "        log = 'restarting pod reflector'\n",
    "    elif code in ['1997', '2780']:\n",
    "        user = msg.split('-')[-1]\n",
    "        log = 'deleting pod'\n",
    "    elif code == '2069':\n",
    "        user = msg.split()[0].replace(',', '')\n",
    "        log = 'user does not appear to be running and shutting it down'  \n",
    "    elif code in ['2077', '2504']:\n",
    "        user = msg.split()[0]\n",
    "        log = 'still running'\n",
    "    elif code == '2085':\n",
    "        user = msg.split()[0]\n",
    "        log = 'server appears to have stopped while the hub was down'\n",
    "    elif code == '2170':\n",
    "        user = msg.split('-')[-1]\n",
    "        log = 'deleting oauth client'\n",
    "    else:\n",
    "        user, log = '', ''\n",
    "    return user, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_parce_users_activities = udf(parce_users_activities, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn('user_act', udf_parce_users_activities(sdf['event_code'], sdf['message']))\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c4408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    F.col('timestamp').alias('jh_timestamp'),\n",
    "    F.col('hub').alias('jh_hub'),\n",
    "    F.col('img').alias('jh_img'),\n",
    "    F.col('host').alias('jh_host'),\n",
    "    F.col('event_code').alias('jh_event_code'),\n",
    "    F.col('event_type').alias('jh_event_type'),\n",
    "    F.col('user_act')[1].alias('jh_log'),\n",
    "    F.col('user_act')[0].alias('jh_user')\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6cf506",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.filter(sdf.jh_user != '')\n",
    "sdf = sdf.withColumn(\n",
    "    'jh_timestamp',\n",
    "    F.to_timestamp(\"jh_timestamp\", \"yyyy-MM-dd HH:mm:ss.SSS\")\n",
    ")\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eea77",
   "metadata": {},
   "source": [
    "### 4. Normalize data (Spark way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a1ebb",
   "metadata": {},
   "source": [
    "#### 4.1. Users table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916debe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logins = sdf.select('jh_user').distinct().collect()\n",
    "print(len(logins))\n",
    "logins = [list(x)[0] for x in logins]\n",
    "logins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c925ce",
   "metadata": {},
   "source": [
    "Use `names` library https://pypi.org/project/names/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9491e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e00de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e4ccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = []\n",
    "for login in logins:\n",
    "    user = {}\n",
    "    user['login'] = login\n",
    "    user['name'] = names.get_full_name()\n",
    "    user['email'] = login + '@gsom.spbu.ru'\n",
    "    users.append(user)\n",
    "users[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ae353",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([users])\n",
    "sdf_users = spark.read.json(rdd)\n",
    "sdf_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbed892",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_users.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225682d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -R /jovyan/users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202e27d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_users.coalesce(1).write.csv('/jovyan/users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661990bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /jovyan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca775de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /jovyan/users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d20b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "file_name=$(hdfs dfs -ls -C /jovyan/users | grep csv)\n",
    "\n",
    "hdfs dfs -head ${file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e56f7a",
   "metadata": {},
   "source": [
    "#### 4.2. JupyterHub instances table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_instances = sdf.select(\n",
    "    'jh_hub',\n",
    "    'jh_img',\n",
    "    'jh_host'\n",
    ")\n",
    "sdf_instances.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8855bda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_instances.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fed488",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_instances = sdf_instances.dropDuplicates()\n",
    "sdf_instances.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0775958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_instances.coalesce(1).write.csv('/jovyan/instances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebdb2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls -C /jovyan/instances | grep csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b3c927",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "file_name=$(hdfs dfs -ls -C /jovyan/instances | grep csv)\n",
    "\n",
    "hdfs dfs -head ${file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cb2d4",
   "metadata": {},
   "source": [
    "#### 4.3. JupyterHub events table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc03b8",
   "metadata": {},
   "source": [
    "### <font color='red'>HOME ASSIGNMENT</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c15999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code will be here\n",
    "# create a table `events` with columns `event_code`, `event_type`, `log`\n",
    "# drop duplicates and save it to CSV file (to HDFS) to import to database later "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9ab5c1",
   "metadata": {},
   "source": [
    "#### 4.4. JupyterHub logs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE that it is an example\n",
    "# you will need to keep only `event_code` column as a key\n",
    "# and remove `event_type` and `log` columns\n",
    "# for data normalization\n",
    "\n",
    "sdf_logs = sdf.select(\n",
    "    'jh_timestamp',\n",
    "    'jh_hub',\n",
    "    'jh_event_code',\n",
    "    'jh_event_type',   # to be removed\n",
    "    'jh_log',          # to be removed\n",
    "    'jh_user'\n",
    ")\n",
    "sdf_logs.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b972b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_logs.coalesce(1).write.option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss.SS\").csv('/jovyan/logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6294f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /jovyan/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "file_name=$(hdfs dfs -ls -C /jovyan/logs | grep csv)\n",
    "\n",
    "hdfs dfs -head ${file_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ab592",
   "metadata": {},
   "source": [
    "### 5. ETL pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351a0a04",
   "metadata": {},
   "source": [
    "ETL will be developed for Hive database in the Hadoop environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c43b76",
   "metadata": {},
   "source": [
    "### 6. Home assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65a1179",
   "metadata": {},
   "source": [
    "Your home assignment for this part is:\n",
    "\n",
    "0. Use `Spark` instead of `Pandas` to process data\n",
    "1. Take large file with data on logs `~/__DATA/IBDT_Spring_2023/topic_1/jhub_logs_large.csv` and put in into HDFS\n",
    "2. Create a table (via `Spark` dataframe) for events (see `4.3. JupyterHub events table`) and save it as `csv` like we did with `users` and `instances` tables (HDFS!)\n",
    "3. Check your script for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd32e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
