{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Big Data Modern Technologies course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC 5: Data lake concept and tools\n",
    "### Part 1. Apache Spark for data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries and credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "creds = access_data(file_path='access_bucket.json')\n",
    "print(creds.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>__IMPORTANT NOTE__</font>\n",
    "1. Do not set credentials (keys, secrets, passwords) explicitly in your code\n",
    "2. Do not print out variables with credentials in ypur code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data files in object storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use test data from VK social platform. Data was collected through [VK API](https://dev.vk.com/reference).\n",
    "\n",
    "There are three groups in the bucket:\n",
    "- gsom_abiturient\n",
    "- gsom_ma\n",
    "- gsom.spbu\n",
    "\n",
    "Data is updated every week, last available data is (names of the folders):\n",
    "- gsom_abiturient-2022-12-20-05-00-20-364747\n",
    "- gsom_ma-2022-12-19-05-00-08-338606\n",
    "- gsom_spbu-2022-12-14-05-00-08-207227"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure in every folder is as follows:\n",
    "- `/walls` folder with walls data of members oh the group\n",
    "- `<GROUP_NAME>.json` (e.g.`gsom_abiturient.json`) file with the group description\n",
    "- `members_full_group_<GROUP_NAME>.json` (e.g. `members_full_group_gsom_abiturient.json`) file with the full data for members of the group\n",
    "- `members_group_<GROUP_NAME>.json` (e.g. `members_group_gsom_abiturient.json`) file with the list of group's members\n",
    "- `wall_owner_id_<GROUP_ID>.json` (e.g. `wall_owner_id_23777199.json`) file with wall of the group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Session and client to access files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[About boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=creds['aws_access_key_id'],\n",
    "    aws_secret_access_key=creds['aws_secret_access_key'],\n",
    "    endpoint_url='https://storage.yandexcloud.net'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VK_DATA_BUCKET = 'apid-data-vk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [key['Key'] for key in s3.list_objects(Bucket=VK_DATA_BUCKET)['Contents']]\n",
    "print('files in storage:', all_files[:10]) # works only for num of files < 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_s3_objects(s3, bucket, upfolder, verbose=False):\n",
    "    s3_result = s3.list_objects_v2(Bucket=bucket, Prefix=upfolder)\n",
    "    loaded = []\n",
    "    if 'Contents' in s3_result.keys():\n",
    "        for key in s3_result['Contents']:\n",
    "            loaded.append(key['Key'])\n",
    "    else:\n",
    "        loaded = []\n",
    "    if verbose: print(f'loaded: {len(loaded)}')\n",
    "    while s3_result['IsTruncated']:\n",
    "        continuation_key = s3_result['NextContinuationToken']\n",
    "        s3_result = s3.list_objects_v2(\n",
    "            Bucket=bucket, \n",
    "            Prefix=upfolder, \n",
    "            ContinuationToken=continuation_key\n",
    "        )\n",
    "        for key in s3_result['Contents']:\n",
    "            loaded.append(key['Key'])\n",
    "        if verbose: print(f'loaded: {len(loaded)}')\n",
    "    return loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = get_all_s3_objects(s3, bucket=VK_DATA_BUCKET, upfolder='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[x for x in all_files if 'gsom_abiturient' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set([x.split('/')[0] for x in all_files if 'gsom_abiturient' in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last data for `gsom_abiturient`\n",
    "sorted(\n",
    "    set([x.split('/')[0] for x in all_files if 'gsom_abiturient' in x]), \n",
    "    reverse=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last data for `gsom_ma`\n",
    "sorted(\n",
    "    set([x.split('/')[0] for x in all_files if 'gsom_ma' in x]), \n",
    "    reverse=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last data for `gsom_spbu`\n",
    "sorted(\n",
    "    set([x.split('/')[0] for x in all_files if 'gsom_spbu' in x]), \n",
    "    reverse=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Load data from the storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_folder = sorted(\n",
    "    set([x.split('/')[0] for x in all_files if 'gsom_ma' in x]), \n",
    "    reverse=True\n",
    ")[0]\n",
    "\n",
    "file_to_load = last_folder + '/gsom_ma.json'\n",
    "print('file to load:', file_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('file to load:', file_to_load)\n",
    "get_object_response = s3.get_object(\n",
    "    Bucket=VK_DATA_BUCKET, \n",
    "    Key=file_to_load\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_object_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(get_object_response['Body'])\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Time for Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Read data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, struct, count_distinct, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web UI for the Spark\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "# Spark settings\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[*]')    # max 5 cores available, use `local[*]` for all cores\n",
    "conf.set('spark.driver.memory', '16G')  # max 16 GB available\n",
    "conf.set('spark.driver.maxResultSize', '4G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Spark's access for object storage settings\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', creds['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', creds['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', \n",
    "                                     'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark can [read JSON](https://spark.apache.org/docs/latest/sql-data-sources-json.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f's3a://{VK_DATA_BUCKET}/' + all_files[3]\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Basic data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schema is your primary info tool for Spark dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf.select('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('profiles').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('count', 'profiles').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(sdf.profiles).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# does not work \n",
    "# why?\n",
    "sdf.select(sdf.count, sdf.profiles).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the answer\n",
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that's better\n",
    "sdf.select(sdf['count'], sdf.profiles).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to deal with `array`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just `explode` them with `pyspark.sql` functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.explode(sdf.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(F.explode(sdf.items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(F.explode(sdf.items)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(F.explode(sdf.groups)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_exploded = sdf.select(F.explode(sdf.groups).alias('groups'))\n",
    "sdf_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_exploded.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_exploded.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many `explode`s\n",
    "# do not work\n",
    "sdf_exploded = sdf.select(\n",
    "    F.explode(sdf.groups),\n",
    "    F.explode(sdf.items)\n",
    ")\n",
    "sdf_exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but you can `explode` one by one\n",
    "# `explode` groups at the first step\n",
    "sdf = sdf.select(\n",
    "    F.explode(sdf.groups).alias('groups'),\n",
    "    sdf.items\n",
    ")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `explode` items at the second step\n",
    "sdf = sdf.select(\n",
    "    sdf.groups,\n",
    "    F.explode(sdf.items).alias('items')\n",
    ")\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to deal with `struct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start from `groups` column\n",
    "sdf.select('groups.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('groups.*').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select('groups.*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(\n",
    "    'groups.*', \n",
    "    'items.*'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.select(\n",
    "    'groups.*', \n",
    "    'items.*'\n",
    ").limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be problems with the 'unstructuring'? For example:\n",
    "- dataframe can contain mixed types of columns (`nested` aka `struct` and plain columns)\n",
    "- same sub-columns names within different structures\n",
    "\n",
    "Function to solve this problem is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_df(df, prefix=None):\n",
    "    \"\"\"\n",
    "    Extracts ata from Struct type colums\n",
    "    of tha Spark dataframe. Function works\n",
    "    with columns and nested or `struct` type\n",
    "    columns, but does not `explode` array \n",
    "    type columns.\n",
    "    \n",
    "    Can take `prefix` to name nested colums.\n",
    "    \n",
    "    \"\"\"\n",
    "    flat_cols = [c[0] for c in df.dtypes if c[1][:6] != 'struct']\n",
    "    nested_cols = [c[0] for c in df.dtypes if c[1][:6] == 'struct']\n",
    "    flat_df = df.select(\n",
    "        flat_cols + \n",
    "        [F.col(ncol + '.' + col).alias(prefix + col if prefix else ncol + '_' + col ) \n",
    "         for ncol in nested_cols \n",
    "         for col in df.select(ncol + '.*').columns]\n",
    "    )\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured = flat_df(sdf, prefix=None)\n",
    "sdf_unstructured.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conversion to timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured = sdf_unstructured.withColumn(\n",
    "    'items_date',\n",
    "    F.to_timestamp('items_date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Value counts and group by operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured.groupBy('items_owner_id').count().orderBy('count').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured.groupBy('items_post_type').count().orderBy('count').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured.groupBy('groups_screen_name').count().orderBy('count').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured.groupBy('items_id').count().orderBy('count').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured.filter(sdf_unstructured.items_id == 2509).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_unstructured.filter(\n",
    "    (sdf_unstructured.items_id == 2509) &\n",
    "    (sdf_unstructured.groups_name.contains('СПбГУ'))\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Data processing for EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read many files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last data for `gsom_abiturient`\n",
    "sorted(\n",
    "    set([x.split('/')[0] for x in all_files if 'gsom_abiturient' in x]), \n",
    "    reverse=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last data for `gsom_ma`\n",
    "sorted(\n",
    "    set([x.split('/')[0] for x in all_files if 'gsom_ma' in x]), \n",
    "    reverse=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last data for `gsom_spbu`\n",
    "sorted(\n",
    "    set([x.split('/')[0] for x in all_files if 'gsom_spbu' in x]), \n",
    "    reverse=True\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass a list of files to Spark reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = [\n",
    "    f's3a://{VK_DATA_BUCKET}/gsom_spbu-2022-11-16-05-00-11-755938/wall_owner_id_*.json',\n",
    "    f's3a://{VK_DATA_BUCKET}/gsom_ma-2022-11-14-05-00-20-220713/wall_owner_id_*.json',\n",
    "    f's3a://{VK_DATA_BUCKET}/gsom_abiturient-2022-11-15-05-00-12-366823//wall_owner_id_*.json'\n",
    "]\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or can pass a mask for files' names to Spark reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_mask = f's3a://{VK_DATA_BUCKET}/gsom_*-2022-11-1*/wall_owner_id_*.json'\n",
    "sdf = spark.read.json(file_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    F.explode(sdf.groups).alias('groups'), \n",
    "    sdf.items\n",
    ")\n",
    "sdf = flat_df(sdf, prefix='')\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(\n",
    "    sdf.groups_id, \n",
    "    sdf.groups_name, \n",
    "    sdf.groups_type,\n",
    "    F.explode(sdf.items).alias('items')\n",
    ")\n",
    "sdf = flat_df(sdf, prefix='')\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupBy('groups_name').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupBy('groups_name', 'groups_type').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.withColumn(\n",
    "    'items_date',\n",
    "    F.to_timestamp('items_date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts = sdf.select(\n",
    "    sdf.items_date,\n",
    "    sdf.items_comments,\n",
    "    sdf.items_likes,\n",
    "    sdf.items_reposts,\n",
    "    sdf.items_views,\n",
    "    sdf.items_text,\n",
    "    sdf.groups_name,\n",
    "    sdf.groups_type\n",
    ").dropDuplicates()\n",
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts = flat_df(sdf_posts, prefix='')\n",
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts = sdf_posts.select(\n",
    "    sdf_posts.items_date,\n",
    "    sdf_posts.items_text,\n",
    "    sdf_posts.items_comments_count,\n",
    "    sdf_posts.items_likes_count,\n",
    "    sdf_posts.items_reposts_count\n",
    ").dropDuplicates()\n",
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. EDA with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Top posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by comments\n",
    "sdf_posts.sort(sdf_posts.items_comments_count.desc()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by likes\n",
    "sdf_posts.sort(sdf_posts.items_likes_count.desc()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by reposts\n",
    "sdf_posts.sort(sdf_posts.items_reposts_count.desc()).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Time trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.sort('items_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# likes by days\n",
    "time_axis = [\n",
    "    x.items_date \n",
    "    for x in sdf_posts.sort('items_date').select('items_date').collect()\n",
    "]\n",
    "likes_count = [\n",
    "    x.items_likes_count \n",
    "    for x in sdf_posts.sort('items_date').select('items_likes_count').collect()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(time_axis, likes_count)\n",
    "plt.ylabel('likes')\n",
    "plt.xlabel('timestamp')\n",
    "plt.title('Likes over the time')\n",
    "plt.legend(['likes'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reposts by days\n",
    "reposts_count = [\n",
    "    x.items_reposts_count \n",
    "    for x in sdf_posts.sort('items_date').select('items_reposts_count').collect()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.plot(time_axis, reposts_count)\n",
    "plt.ylabel('likes')\n",
    "plt.xlabel('timestamp')\n",
    "plt.title('Reposts over the time')\n",
    "plt.legend(['reposts'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments by day of week\n",
    "sdf_posts = sdf_posts.withColumn('items_date_dow', F.dayofweek('items_date'))\n",
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dow = sdf_posts.groupBy('items_date_dow').sum().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_dow = [(x['items_date_dow'], x['sum(items_comments_count)']) for x in comments_dow]\n",
    "comments_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow = [x[0] for x in comments_dow]\n",
    "comments = [x[1] for x in comments_dow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(dow, comments)\n",
    "plt.ylabel('comments')\n",
    "plt.xlabel('day of week')\n",
    "plt.title('Comments bt day of the week')\n",
    "plt.legend(['comments'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reposts vs likes\n",
    "likes = [\n",
    "    x.items_likes_count \n",
    "    for x in sdf_posts.sort('items_date').select('items_likes_count').collect()\n",
    "]\n",
    "reposts = [\n",
    "    x.items_reposts_count \n",
    "    for x in sdf_posts.sort('items_date').select('items_reposts_count').collect()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(likes, reposts)\n",
    "plt.ylabel('reposts')\n",
    "plt.xlabel('likes')\n",
    "plt.title('Reposts vs Likes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comments and likes\n",
    "comments = [\n",
    "    x.items_comments_count \n",
    "    for x in sdf_posts.sort('items_date').select('items_comments_count').collect()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.scatter(likes, comments)\n",
    "plt.ylabel('comments')\n",
    "plt.xlabel('likes')\n",
    "plt.title('Comments vs Likes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. Word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word count task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the power of [Spark's RDDs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need RDD for the `word count` task\n",
    "rdd_posts = sdf_posts.limit(1000).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 records\n",
    "rdd_posts.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get text field from RDD\n",
    "rdd_posts.take(5)[4].items_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only posts' texts\n",
    "rdd_posts.map(lambda x: x.items_text).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to collection of words\n",
    "rdd_posts.map(lambda x: x.items_text) \\\n",
    "    .flatMap(lambda line: line.split(' ')) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `map` function (like MapReduce algorithm)\n",
    "rdd_posts.map(lambda x: x.items_text) \\\n",
    "    .flatMap(lambda line: line.split(' ')) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and `reduce` function too\n",
    "rdd_posts.map(lambda x: x.items_text) \\\n",
    "    .flatMap(lambda line: line.split(' ')) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort to find more frequent words\n",
    "rdd_posts.map(lambda x: x.items_text) \\\n",
    "    .flatMap(lambda line: line.split(' ')) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .sortBy( lambda x: x[1] , ascending=False) \\\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final solution with no short words\n",
    "rdd_posts = rdd_posts.map(lambda x: x.items_text) \\\n",
    "    .flatMap(lambda line: line.split(' ')) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .sortBy( lambda x: x[1] , ascending=False) \\\n",
    "    .filter(lambda x: len(x[0]) > 3)\n",
    "rdd_posts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resulting diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = rdd_posts.collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freqs = {x[0]: x[1] for x in result}\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freqs_bar = {k: v for k, v in freqs.items() if 50 <= v <= 1000}\n",
    "freqs_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "plt.bar(*zip(*freqs_bar.items()))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Home assignment (optionally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your home assignment for this part is to repeat all the steps but with new data e.g. from the wall of any group. \n",
    "\n",
    "As an example you can take data like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_mask = f's3a://{VK_DATA_BUCKET}/gsom_ma-2022-11-14-05-00-20-220713/walls/wall_owner_id_*.json'\n",
    "sdf = spark.read.json(file_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
