{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Big Data Modern Technologies course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC 5: Data lake concept and tools\n",
    "### Part 2. Apache Spark for ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "creds = access_data(file_path='access_bucket.json')\n",
    "print(creds.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>__IMPORTANT NOTE__</font>\n",
    "1. Do not set credentials (keys, secrets, passwords) explicitly in your code\n",
    "2. Do not print out variables with credentials in ypur code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Browse files at S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=creds['aws_access_key_id'],\n",
    "    aws_secret_access_key=creds['aws_secret_access_key'],\n",
    "    endpoint_url='https://storage.yandexcloud.net'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTS_DATA_BUCKET = 'apid-data-options'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [key['Key'] for key in s3.list_objects(Bucket=OPTS_DATA_BUCKET)['Contents']]\n",
    "print('files in storage:', all_files[:10]) # works only for num of files < 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data preprocessing with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, struct, count_distinct, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# web UI for the Spark\n",
    "\n",
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "# Spark settings\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[*]')    # max 5 cores available, use `local[*]` for all cores\n",
    "conf.set('spark.driver.memory', '16G')  # max 16 GB available\n",
    "conf.set('spark.driver.maxResultSize', '4G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# Spark's access for object storage settings\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', creds['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', creds['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.aws.credentials.provider', \n",
    "                                     'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Read base data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data refers to [Options Stock markets](https://www.investopedia.com/terms/o/option.asp) and contains gigabytes of the data from foreign stock markets. The data is kindly provided by VTB experts for educational purposes only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 1 year for the start\n",
    "file_path = f's3a://{OPTS_DATA_BUCKET}/' + 'data/L3_options_2016*.parquet'\n",
    "options = spark.read.parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = options.agg({'date': 'min'}).collect()[0].asDict()['min(date)']\n",
    "max_date = options.agg({'date': 'max'}).collect()[0].asDict()['max(date)']\n",
    "print('from', min_date, 'to', max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Filter by assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_count = (\n",
    "    options\n",
    "        .groupBy('base_symbol')\n",
    "        .count()\n",
    "        .orderBy('count', ascending=False)\n",
    ")\n",
    "assets_count.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_selected = assets_count.limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_selected[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_selected = [x.asDict()['base_symbol'] for x in assets_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def check_if_out_of_money(option_type, base_price, strike):\n",
    "    if option_type == 'call' and base_price < strike:\n",
    "        return 1\n",
    "    elif option_type == 'call' and base_price >= strike:\n",
    "        return 0\n",
    "    elif option_type == 'put' and base_price > strike:\n",
    "        return 1\n",
    "    elif option_type == 'put' and base_price <= strike:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_add_cols = (\n",
    "    options \n",
    "        .filter(F.col('base_symbol').isin(assets_selected))\n",
    "        .withColumn('date_parsed', F.to_date(F.col('date'), 'MM/dd/yyyy')) \n",
    "        .withColumn('day', F.dayofmonth(F.col('date_parsed'))) \n",
    "        .withColumn('month', F.month(F.col('date_parsed'))) \n",
    "        .withColumn('year', F.year(F.col('date_parsed'))) \n",
    "        .withColumn('exp_date_parsed', F.to_date(F.col('expiration'), 'MM/dd/yyyy')) \n",
    "        .withColumn('days_diff', F.datediff(F.col('exp_date_parsed'), F.col('date_parsed'))) \n",
    "        .withColumn('weeks_diff', F.col('days_diff') / 7) \n",
    "        .withColumn('bid_ask_mean', (F.col('bid') + F.col('ask')) / 2) \n",
    "        .withColumn('is_call_option', (F.col('type') == 'call').cast(IntegerType())) \n",
    "        .withColumn('strike_over_base', F.col('strike') / F.col('base_price')) \n",
    "        .withColumn(\n",
    "            'out_of_money', check_if_out_of_money(\n",
    "                F.col('type'),\n",
    "                F.col('base_price'),\n",
    "                F.col('strike')\n",
    "            ).cast(IntegerType())\n",
    "        )\n",
    "        .drop('date', 'expiration', 'aka') \n",
    "        .withColumnRenamed('exp_date_parsed', 'expiration_date') \n",
    "        .withColumnRenamed('date_parsed', 'date') \n",
    "        .select(\n",
    "            'base_symbol',\n",
    "            'base_price',\n",
    "            'option_symbol',\n",
    "            'type',\n",
    "            'is_call_option',\n",
    "            'date',\n",
    "            'expiration_date',\n",
    "            'days_diff',\n",
    "            'bid_ask_mean',\n",
    "            'strike',\n",
    "            'strike_over_base',\n",
    "            'out_of_money',\n",
    "            'volume',\n",
    "        )\n",
    "        .orderBy('date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_add_cols.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_add_cols.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Volatilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data = (\n",
    "    options\n",
    "        .select(\n",
    "            'base_symbol',\n",
    "            'base_price',\n",
    "            'date'\n",
    "        )\n",
    "        .withColumn('date_parsed', F.to_date(F.col('date'), 'MM/dd/yyyy'))\n",
    "        .drop('date')\n",
    "        .withColumnRenamed('date_parsed', 'date')\n",
    "        .groupBy('base_symbol', 'date')\n",
    "        .agg(\n",
    "             F.first('base_price').alias('base_price')\n",
    "        )\n",
    "        .orderBy('date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp is interpreted as UNIX timestamp in seconds\n",
    "days = lambda x: x * 86400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = (Window()\n",
    "      .partitionBy(F.col('base_symbol'))\n",
    "      .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "      .rangeBetween(-days(1 + 1), -days(1)))\n",
    "d2 = (Window()\n",
    "      .partitionBy(F.col('base_symbol'))\n",
    "      .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "      .rangeBetween(-days(2 + 1), -days(1)))\n",
    "\n",
    "# HOME ASSIGNMENT\n",
    "# create `d3` (three days before lag) \n",
    "# try to use folowing code:\n",
    "# d3 = (Window()\n",
    "#       .partitionBy(F.col('base_symbol'))\n",
    "#       .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "#       .rangeBetween(-days(<DAYS_BEFORE_LAG> + 1), -days(1)))\n",
    "\n",
    "w1 = (Window()\n",
    "      .partitionBy(F.col('base_symbol'))\n",
    "      .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "      .rangeBetween(-days(7 + 1), -days(1)))\n",
    "\n",
    "# HOME ASSIGNMENT\n",
    "# w2(two weeks before lag)\n",
    "# try to use folowing code:\n",
    "# w2 = (Window()\n",
    "#       .partitionBy(F.col('base_symbol'))\n",
    "#       .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "#       .rangeBetween(-days(<DAYS_BEFORE_LAG> * <NUM_WEEKS_BEFORE_LAG> + 1), -days(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_volatilities = (\n",
    "    stocks_data\n",
    "        .withColumn('1d_mean', F.mean('base_price').over(d1))\n",
    "        .withColumn('2d_mean', F.mean('base_price').over(d2))\n",
    "        # HOME ASSIGNMENT\n",
    "        # add `mean` column for `d3`\n",
    "        # you may use code like:\n",
    "        # .withColumn('3d_mean', F.mean('base_price').over(<WINDOW_D3>))\n",
    "        .withColumn('1w_mean', F.mean('base_price').over(w1))\n",
    "        # HOME ASSIGNMENT\n",
    "        # add `mean` column for `w2`\n",
    "        # <YOUR_CODE_HERE>\n",
    "        .withColumn('1d_std', F.stddev('base_price').over(d1))\n",
    "        .withColumn('2d_std', F.stddev('base_price').over(d2))\n",
    "        # HOME ASSIGNMENT\n",
    "        # add `std` column for `d3`\n",
    "        # <YOUR_CODE_HERE>\n",
    "        .withColumn('1w_std', F.stddev('base_price').over(w1))\n",
    "        # HOME ASSIGNMENT\n",
    "        # add `std` column for `w2`\n",
    "        # <YOUR_CODE_HERE>\n",
    "        .withColumn('1d_volatility', F.col('1d_std') / F.col('1d_mean'))\n",
    "        .withColumn('2d_volatility', F.col('2d_std') / F.col('2d_mean'))\n",
    "        # HOME ASSIGNMENT\n",
    "        # add `volatility` column for `d3`\n",
    "        # <YOUR_CODE_HERE>\n",
    "        .withColumn('1w_volatility', F.col('1w_std') / F.col('1w_mean'))\n",
    "        # HOME ASSIGNMENT\n",
    "        # add `volatility` column for `w2`\n",
    "        # <YOUR_CODE_HERE>\n",
    "        .select(\n",
    "            'base_symbol',\n",
    "            'date',\n",
    "            '1d_mean',\n",
    "            '2d_mean',\n",
    "            '1w_mean',\n",
    "            '1d_volatility',\n",
    "            '2d_volatility',\n",
    "            '1w_volatility'\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_volatilities.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = stocks_volatilities.agg({'date': 'min'}).collect()[0].asDict()['min(date)']\n",
    "max_date = stocks_volatilities.agg({'date': 'max'}).collect()[0].asDict()['max(date)']\n",
    "print(min_date, max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = min_date + datetime.timedelta(weeks=2) #datetime.date(2017, 1, 1)\n",
    "end_date = max_date\n",
    "\n",
    "features = (\n",
    "    options_add_cols.join(stocks_volatilities, on=['base_symbol', 'date'], how='left')\n",
    "        .filter(F.col('date') > start_date)\n",
    "        .filter(F.col('date') <= end_date)\n",
    "        .orderBy('date')\n",
    "        .drop('option_symbol', 'expiration_date', 'type', 'date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. External data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ~/__DATA/IBDT_Spring_2024/topic_5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = spark.read.csv(\n",
    "    '/home/jovyan/__DATA/IBDT_Spring_2024/topic_5/Sector_Industry_Country_MarketCap.csv',\n",
    "    sep=',', \n",
    "    header=True\n",
    ")\n",
    "markets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors_dummy = markets.groupBy(\"Ticker\").pivot(\"Sector\").agg(F.lit(1)).na.fill(0)\n",
    "sectors_dummy.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_dummy = markets.groupBy(\"Ticker\").pivot(\"Country\").agg(F.lit(1)).na.fill(0)\n",
    "countries_dummy.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Add external data and finalizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.join(sectors_dummy, features.base_symbol == sectors_dummy.Ticker)\n",
    "features = features.drop(features.Ticker)\n",
    "features.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.join(countries_dummy, features.base_symbol == countries_dummy.Ticker)\n",
    "features = features.drop(features.Ticker)\n",
    "features.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that is what we are trying to predict\n",
    "y_col = 'bid_ask_mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [x for x in features.columns if x not in ['base_symbol', y_col]]\n",
    "print(x_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modelling with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor, LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(\n",
    "    inputCols=x_cols,     # all X columns except y-column\n",
    "    outputCol='features'  # name for assembled rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vec = vecAssembler.transform(features)\n",
    "features_vec.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = features_vec.select('bid_ask_mean', 'features')\n",
    "features_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = features_data.withColumnRenamed('bid_ask_mean', 'label')\n",
    "features_data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will use random forest\n",
    "rf = RandomForestRegressor(\n",
    "    labelCol='label', \n",
    "    featuresCol='features'\n",
    ")\n",
    "\n",
    "# HOME ASSIGNMENT\n",
    "# you may want to implement linear regression\n",
    "# with the following code:\n",
    "# lr = LinearRegression(\n",
    "#     labelCol=<LABEL_COLUMN>, \n",
    "#     featuresCol=<FEATURES_COLUMN>\n",
    "# )\n",
    "\n",
    "# pipeline may include many steps to prepare data\n",
    "pipeline = Pipeline(stages=[rf])\n",
    "\n",
    "# HOME ASSIGNMENT\n",
    "# you may want to implement new \n",
    "# pipeline for linear regression\n",
    "# with the following code:\n",
    "# pipeline = Pipeline(stages=[<YOUR_NEW_REGRESSOR>])\n",
    "\n",
    "# search for best parameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [5, 10]) \\\n",
    "    .addGrid(rf.maxDepth, [1, 2]) \\\n",
    "    .build()\n",
    "\n",
    "# HOME ASSIGNMENT\n",
    "# you may want to implement new \n",
    "# parameters grid search \n",
    "# with the following code:\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#     .addGrid(<PARAMETER_TO_TUNE>, <LIST_OF_VALUES>) \\\n",
    "#     .build()\n",
    "\n",
    "# cross-validation strategy\n",
    "cross_val = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(),\n",
    "    numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# takes about 5 min\n",
    "\n",
    "feat_train, feat_test = features_data.randomSplit([.8, .2], seed=2024)\n",
    "model = cross_val.fit(feat_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(feat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WMAPE(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.sum(np.abs(y_true - y_pred)) / np.sum(y_true) * 100\n",
    "\n",
    "wmape = WMAPE(df.label.to_list(), df.prediction.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(\n",
    "    df.label.to_list(), \n",
    "    df.prediction.to_list(), \n",
    "    'bo'\n",
    ")\n",
    "plt.xlabel('Bid-ask mean price')\n",
    "plt.ylabel('Prediction')\n",
    "plt.suptitle(f'RMSE: {rmse:.1f},  WMAPE: {wmape:.1f}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Home assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your home assignment will be to implement `LinearRegression` to predict `bid_ask_mean` as it was done with Random Forest regression model. Here is [a manual for linear regression with Spark ML](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html). Note, that you have to tune only one parameter `regParam` for the linear regression model in the `ParamGridBuilder`.\n",
    "\n",
    "You will also have to:\n",
    "- use data for year 2017 e.g. `file_path = f's3a://{OPTS_DATA_BUCKET}/' + 'data/L3_options_2017*.parquet` for the data to load (year 2017 has more observations)\n",
    "- add more features for stock volatilities in `3.4. Volatilities`: `d3` (three days before lag) and `w2`(two weeks before lag) to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
